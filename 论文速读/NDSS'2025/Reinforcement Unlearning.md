#开源
# Authors
Dayong Ye，应该是华人，悉尼科技大学和CSIRO 61的联合作品

# Abstract
背景：强化学习领域仍未有人关注遗忘这一重要领域。在训练种，智能体agent会记住环境environment的特征，这会导致隐私问题。因此作者提出了**强化遗忘**这一新术语。

动机：强化遗忘旨在遗忘整个环境，而非某些数据样本。因此有三个问题需要解决：①怎么遗忘环境 ②怎么避免智能体在剩余环境性能下降 ③怎么评估遗忘有效性

方法：提出两种解决方案，一是基于减量强化学习，逐渐擦除智能体获得的知识。二是利用投毒攻击，鼓励智能体学习新的（尽管不正确的）知识来遗忘旧的知识。此外整了个environment inference当作评估指标

# Introduction
第一段，介绍MU
第二段，MU只在传统领域有发展，忽视了RL领域。介绍RL基本流程。
第三段，然而RL的agent可能学到不该学的信息，例如地图的受限区域。
第四段，另一个例子：RL-based推荐系统会记住用户敏感信息。用户应可以退出或请求删除数据，称之为Reinforcement Unlearning。
第五段，传统的MU不适用于RU。MU是删除静态数据，RU则是一个有时序性的动态决策过程。
第六段，RU也不同于隐私保护的RL，一个是删除学到的知识，一个是保护信息。三个挑战：①怎么遗忘环境 ②怎么避免智能体在剩余环境性能下降 ③怎么评估遗忘有效性
1. 动态环境而非静态样本，挑战在于将需要忘掉的环境与相应的体验样本相关联。
2. RU更有挑战性，因为要忘记大量经验样本
3. MU常用的是MIA，RU不能用这个方法，因为环境所有者无法指定应该取消学习哪些样本

第七段，为解决三个挑战，提出了两种方法：减量RL和环境投毒。减量应该是消除整个环境的，当环境确定过时的时候使用。投毒是针对某一部分环境，其他环境不受影响。引入环境推理environment inference作为衡量指标，遗忘后推理结果明显下降视为成功。

贡献：
1. 提出RU
2. 揭示了RL可能会泄露环境的信息，并引入两种创新的RU方法，减量RL，环境投毒。
3. 提出了环境推理指标。


# Conclusion
贡献。
将来想弄个统一的RU框架。