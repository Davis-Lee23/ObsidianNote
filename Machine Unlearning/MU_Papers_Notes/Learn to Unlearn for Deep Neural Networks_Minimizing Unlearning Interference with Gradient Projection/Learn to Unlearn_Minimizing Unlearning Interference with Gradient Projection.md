### 动机
MU存在一大挑战：如何在不更改剩余数据集的前提下，相比从头训练更高效地实现遗忘数据。本文提出了Projected-Gradient Unlearning(PGU)梯度投影遗忘方法，模型在正交方向上对被认为对保留数据集不重要的梯度子空间采取措施，以便保留其知识。该方法可以：
+ 高效地应用于任何模型和任意大小数据集
+ 适用于不可再被访问的数据集
+ 可以用于de-poisoning

#### 解决思路
首先，引入一种新的分类遗忘损失，旨在能reverse遗忘数据的原始训练过程。 
随后，受他人工作启发，对保留数据集的模型权重的核心梯度子空间采用正交梯度下降。
在该方法中，会将梯度空间划分为Core Gradient Space(CGS)核心梯度空间和Residual Gradient Space(RGS)残差梯度空间。其中CGS可以保存必要信息，最小程度干扰保留数据集，以实现避免灾难性遗忘。为此作者还提出了一种对CGS的高效算法，可以让算法算法在无法再次访问数据集的情况下发挥作用。
+ 对于差分隐私(DP)，作者认为DP适用于更加严格的隐私保护场景
+ 作者用Membership Inference Attack (MIA)来检测遗忘效果

### 技术路线
D记作数据集，Df为遗忘数据，Dr为剩余数据集，Df∩Dr=null，Df∪Dr=D

利用SGD更新位于输入数据点的跨度的属性，通过CGS的正交梯度更新来选择性的遗忘部分数据集。

#### 损失函数


### 实验结果
#### 实验设置
数据集与模型：CIFAR-10搭配AllCNN、SmallVGG模型，TinyImageNet搭配ResNet-18模型

#### 评价指标
1. 测试集Dtest错误率：越小越好
2. 被遗忘的子集Df错误率：与无Df参与的重模型越接近越好
3. 剩余数据集Dr错误率：遗忘后变化不大或接近于重训练模型
4. MIA：用多组模型对比，采用ROC和AUC曲线(AUC越接近点(0,1)越好)
5. 模型置信度：以熵的形式可视化展示，即熵越小越好。
6. 遗忘时间：应显著小于重训练时间

#### Data Removal
没有复现AllCNN的代码，此处仅作参考，以再训练的指标作为baseline
+ Dr：PGU，EUk，SCRUB都可以
+ Df：PGU优于其他对比方法(此处EUk方法非常糟糕)
+ Dtest：PGU优于其他方法
+ Times：PGU显著减少
下方熵图中PGU最接近于重训练模型，此外EUk熵图更接近于没有遗忘的柱状图(即暗示EUk无效果)
![[Pasted image 20240224111229.png]]
