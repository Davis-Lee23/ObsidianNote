机器学习的几个要素：
通常会把问题转化为期望风险转化为最小值问题
![[Pasted image 20240704141754.png|500]]

SGD也叫做增量梯度下降法
Mini-Batch随机梯度下降法，随机选取一部分训练样本训练
提前停止法：到达预设acc就停止（可以防止过拟合）


过拟合Overfitting：把训练样本学的“太好”，把训练样本的特点当作了所有样本的一般性质，导致泛化下降
欠拟合Underfitting：对训练样本的一般性质都尚未学好


正则化（Regularization）

偏差-方差分解（Bias-Variance Decomposition）：最小化期望错误等价于最小化偏差和方差之和

### 数学基础
![[Pasted image 20240704144120.png|300]]

#### 线性代数
标量scalar
向量vector
矩阵matrix
张量tensor

**矩阵运算**
转置
矩阵加法、乘法
逆矩阵运算

范数：表示向量长度的函数，赋予向量元素非0的大小
l1范数：向量各元素的绝对值之和
l2范数：向量各元素的平方和再开方
![[Pasted image 20240704144612.png]]

#### 微积分
导数：曲线的斜率，反应曲线变化的快慢
高阶导数：对导数的继续求导
偏导数：对一个变量求导、其它变量不变

泰勒公式：已知某一点的各阶导数值的情况下，可以用这些导数值做系数构建一个多项式来近似函数在这点的邻域中的值。

梯度与导数

<font color="#ff0000">矩阵微分：</font>

链式法则：涉及标量、向量、矩阵

#### 概率和统计
概率公式：
条件概率：
贝叶斯公式：

常见的概率分布：
离散随机分布：伯努利分布、二项分布（更广义的伯努利）
连续随机分布：均匀分布、正态分布

Jensen不等式

大数定理

随机过程

信息熵

### 常用模型
线性模型


Logistic函数与回归

熵Entropy，用来衡量一个随机事件的不确定性，得去了解一下期望值。
总感觉这个熵编码在哪里见过
![[Pasted image 20240704152224.png|525]]

交叉熵

KL散度：KL散度是用概率分布q来近似p时所造成的信息损失量

似然

![[Pasted image 20240704153134.png|500]]

多分类问题：
三种判别方法：一对其余、一对一、argmax属于改进的一对其余
![[Pasted image 20240704153550.png|550]]


Softmax回归
![[Pasted image 20240704154319.png|400]]

感知机 Perceptron
模拟生物神经元行为的机器

感知机收敛性证明
![[Pasted image 20240704155048.png|400]]