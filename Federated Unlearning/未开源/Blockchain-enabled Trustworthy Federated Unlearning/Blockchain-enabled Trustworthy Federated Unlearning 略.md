### Motivation
中央服务器在客户端退出后依然可以利用模型历史参数进一步训练。提出一种区块链可信联邦遗忘框架，设计了一个利用变色龙哈希函数进行遗忘验证的联邦遗忘协议(如何遗忘的)，并提出了一种自适应的基于贡献的再训练机制，减少计算开销，提高效率。

#### 完整点
现有工作聚焦于存储-校正，牛顿模型更新，梯度上升等方法。
部分方法要求中央存储历史参数，这可能导致中央服务器在客户端退出后依然可以利用模型历史参数进一步训练，对于验证中央是否又采用了目标客户端的参数验证是一大难点。此外难以确定最优的遗忘轮次和减少计算开销。
提出一种BCFU去验证遗忘，设计了变色龙哈希函数消除目标客户端历史影响，并提出了一种自适应的基于贡献的再训练机制，减少计算开销，提高效率。

#### 贡献小结
1. 提出了一个BCFU，利用智能合约+哈希影响无缝处理连续的遗忘请求（Really？），并验证是否确实删除
2. 基于变色龙的FU证明协议，目标客户端可以调用遗忘函数进行擦除（SC？）
3. 自适应基于贡献的再训练

相关工作：MU，FU，基于区块链的FL证明

### Method

### Experiment

#### 实验设置：
区块链：Xuperchain
OS：Ubuntu 16
数据集：MNIST，FMNIST，CIFAR-10
模型
1. 对于M，2 Conv.+2 Pool. + 2FC的 
2. 对于C，2卷积+2池化+3FC
超参数：50客户端，r=0.5，遗忘间隔2，全局40轮，局部十轮，学习率0.1
基线：FedAvg，FedEraser，RapidTrain(牛顿法)
Metric：acc，loss，MIA的俩，重训练时间
#### 实验结果
acc和loss差不多，MIA指标各有千秋，时间上本文最快。Deviation看不懂

### Pros and Cons
优点：
1. 结合了BC+FU
2. 用了变色龙哈希，可编辑的区块链，不过感觉这有些违背祖宗。
3. 写一堆公式证明有效，但是我没看

缺点：
1. 性能没什么变化，提升聊胜于无
2. 有一些莫名其妙的实验

### Comments
变色龙部分没看不予评价，感觉整篇文章就只有一个BCFU结合是亮点