# 摘要
动机：现有FU方法可以被分成两类且各自有缺点。一类是exact unlearning，依赖于分布式分区聚合，这种方法理论上不会提高效率。另一类是approximate unlearning，该类方法可能受不精确数据影响或有巨大的计算开销，甚至二者皆有。

方法：因此提出了一个**基于增量学习**的FU框架，既不依赖于类似重训的思路，也不受数据影响。**利用新的记忆去重写旧的记忆**，准确的说该方法让学生模型不断从随机的教师模型学东西，并且为了防止灾难性遗忘，还设计了一个弹性权重consolidation去限制权重变化

结果：在三个数据集上高效且有效。在后门实验中达到了satisfying completeness。


# 实验




