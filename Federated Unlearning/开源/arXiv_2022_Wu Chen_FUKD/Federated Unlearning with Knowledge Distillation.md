### 摘要
动机：训练好的模型会记住一些训练的信息，违背了被遗忘权
方法：提出了一种减去累积历史更新，并利用知识蒸馏来重构模型的方法。该方法无需从客户端获取任何数据，不依赖剩余客户端参与。
结果：有效且高效


### 方法
核心思想是①先用累积的历史更新删除目标客户端更新 ②知识蒸馏恢复
4.1 pass
4.2 先介绍了知识蒸馏的起源。将全局模型视作teacher model，skewed遗忘模型视作student model。


### 实验
实验结果表明该遗忘策略可以去掉**目标客户端（攻击者）** 的贡献，并可以用知识蒸馏快速恢复性能。

#### 实验设置
数据集：MNIST/CNN，CIFAR-10/VGG11，GTSRB/AlexNet
客户端：10个，其中1个为攻击者。GTSRB为5 | 1
数据处理：见原文，正则、增强
Tip：还需要采样一些非客户端的数据用于蒸馏。

#### 实验结果
在遗忘的时候，acc会降低到60以下，后门为0

总共的结果如下:UL是unlearning，post是蒸馏之后继续训练
![[Pasted image 20240529100101.png]]


### 展望
训练中结合蒸馏，没有任何数据集的蒸馏