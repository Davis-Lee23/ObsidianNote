
# 摘要
动机：被遗忘权。现有机器遗忘方法需要访问数据，但这不适用于联邦场景，因此提出一个不用访问原始数据的遗忘方法。

方法：
1. 引入了一个unlearning rate去trade-off遗忘数据和原始模型，令其可以适应不同遗忘任务。
2. 引入了BFU with parameter self-sharing（BFU-SS）参数自共享的BFU。BFUSS将数据擦除与保持准确率视为两个任务，并在遗忘中优化它们

结果：又干掉SOTA了


# 方法
应该是要服务器+客户端合作

# 实验
### 实验设置
**数据集：**
MNIST/BNN/η=0.001/E=10/G=10
CIFAR-10/ResNet-18/η=0.0005/E=2/G=40
客户端：10个
bs=100
固定了什么Cr = Cu = 1
硬件：1080Ti
指标：时间、acc、L2-norm、KLD，后门（方法好像不太一样）
pipeline：注入后门——基于BFL训练——遗忘
比较方法：HFU，hessian-based FU

### 实验场景
##### 6.2实验
遗忘学习率0.1
MNIST：俩投毒，比例都是10%
Cifar-10：仨投毒

6.2 是中央服务器测试