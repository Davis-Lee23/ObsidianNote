通过知识覆盖进行遗忘：通过选择性稀疏适配器的可逆FU

#CCF-A #CVPR #开源 
# Authors
Zhengyi Zhong，国防科技大学学生。
主要团队是国防科大，末作背书有日本索尼和南洋理工

# Abstract
背景：FL很好，但在实践中我们不仅要保证学习，也要保证RTBF
动机：当前FU有几个缺点，<font color="#ff0000">无差别遗忘客户端知识，遗忘不可逆，成本高</font>
方法：FUSED。首先分析不同层对知识的敏感性，为敏感的层构建稀疏的学习适配器（怎么分析敏感性，敏感的标准是什么）。然后在不更改原始参数的情况下训练适配器，用剩余的知识覆盖忘却知识。

结果：这种知识覆盖使得FUSED减轻了无差别遗忘的影响，独立的适配器让遗忘可逆并且大幅减少了开销。在三个数据集上证明有效

# Introduction
背景：引入了一个目标客户端主动遗忘的案例（纽约时报和GPT），还有有害数据需要删除的需求。

现有方法主要挑战：
无差别遗忘：在客户端之间存在知识重叠的情况下，传统方法在遗忘过程中会无差别地移除共享知识，导致其他客户端的性能大幅下降。（在adaptor上用剩余数据训练，避免无差别遗忘）

不可逆遗忘：在FL系统中，客户端的遗忘请求可能会动态变化。当客户端不再需要遗忘某些知识时，传统方法无法快速恢复该记忆。

显著的遗忘成本：基于重训练的方法需要多次迭代，导致计算和通信成本显著增加。即使是简单的模型参数调整，也可能需要大量的存储作为补偿成本。


# Method
左边就是CLI的识别，右边就是稀疏适配器的重构
![[Pasted image 20250424005141.png]]

## 步骤一 CLI：Critical layer identification
用曼哈顿距离来做测量，变化最大的就是最敏感的层，最小的就是鲁棒的层。具体测量方式有点迷？

## 步骤二 用稀疏适配器遗忘
根据预设的要被遗忘的层数K进行选择。选出层后，随机遗忘一些参数形成稀疏参数矩阵，成为adaptor。然后就冻结其他层，分发这些层去训练，最后再连接回来。

问题：<font color="#ff0000">为什么要稀疏化？完整的效果不好还是参数量太大？</font>


# Experiment
<font color="#ff0000">挺常规的，就是用的模型还有这个交叉知识干扰挺别致</font>
<font color="#ff0000">对于ReA我持怀疑态度</font>
## Setup
软硬件：PyTorch 2.2.0，4090
超参数：bs=128，客户端数量50，优化器用了SGD和Adam，数据划分迪利克雷的0.1和1，主要结果是1的。
数据集：FMNIST-LeNet，Cifar100-ResNet18，Cifar10-ResNet18与transformer的SimpleViT
基线：Retrain，FedEraser，Exact-Fun（2023-ICDM-B），EraserClient/PGA（2022-ICML-A）

指标：
1. RA：剩余准确率，越高越好
2. FA：目标准确率，越低越好
3. Comp：时间
4. Comm：单个客户端数据传输量
5. MIA：此处是说越低越好（两种流派，一种接近重训，一种越低越好）
6. ReA：?重学习准确率是个什么玩意。AI解释：彻底遗忘后，重新学习会像学习全新内容一样困难，就说如果忘得不彻底，学的就快，但是学得快有没有可能是我模型好？反正就是越低越好

## 关键层识别
果然识别到了FC层

## 结果分析
### 遗忘性能
用标签反转攻击来测试RA和FA。这个不用想肯定好。在Transformer的模型中，FUSED表现出来比重训还好的性能，但是重训才33%，说明没训练好。

### 知识干扰
设置：将标签1和90%的标签0给目标客户端，10%给其他客户端。
对于独立的知识标签1，所有方法都不赖，对于重叠的知识标签0，FUSED接近于重训。（你这自适应啊，一会越低越好，一会越接近重训越好），话说是不是压低了FedEraser的Acc了？

### 遗忘成本
用那些适配器肯定低

### 隐私保护
MIA值

### 消融实验
为了证明CLI的必要性，进行了CLI和随机选层的实验，文章说证明了遗忘效果被加强，我看着像加强了剩余ACC。

# 总结
针对开篇的三个问题：一和三我能理解，可逆的FU什么意思？
代码有开源可以做一个可用的baseline，属于类重训的FU方法

和我的方法的异同：
	同：都用了模型微调，都依赖于剩余客户端的参与。
	异：
	1. 没交代训练阶段，没交代遗忘阶段后训练轮次
	2. 后训练我是有校准操作，它直接训练
	3. 鉴于2，它也修改了MIA的标准，但话又说回来遗忘对于MIA仍未统一
	4. FedHydra还考虑了遗忘时的攻击，本文单纯提出一种遗忘方法