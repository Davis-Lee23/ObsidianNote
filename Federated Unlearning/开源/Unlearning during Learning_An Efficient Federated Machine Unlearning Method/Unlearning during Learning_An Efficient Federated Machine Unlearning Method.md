#CCF-A #开源 #IJCAI #略读 

# Comments
还没跑代码，有几个问题：
1. 时间统计有问题，看伪代码应该是剩余客户端和目标客户端都跑，训练结束之后服务器再执行一个线性操作，客观来说是不是T轮之内都要纳入统计时间？不然Eraser也可以说校准才是我花的时间，其他时候都不计算在内。
2. 代码介绍里把模型最后一层当作辅助器，真的行吗？

# Authors
一作Hanlin Gu，杨强学生，rising star，目前是微众研究院
后续作者包含电科、清深、南阳等强力团队
通讯是杨强

# Abstract 
背景：为响应RTBF，FMU诞生
动机：当前的FMU方法需要额外消耗时间且未必适用多种场景，不实用。
方法：FedAU（auxiliary unlearning），引入了一个轻量级的遗忘模块可以线性的进行遗忘。
结果：该方法消除了对时间的额外需求，适应性强。在多个数据集已经验证

# Introduction
+ FU的定义和重要性
+ 举例三种典型的FU，首先是重训微调类如FedEraser，其次是梯度上升类，再次是用模型剪枝。毫无疑问的是FU有两个重要的评价指标：遗忘时间和遗忘的适用范围。
+ 然而现有方法难以顾及两个方面。本文的方法有三大优势：一是用简单的线性操作实现unlearning减少了时间消耗。二是支持样本、类、客户端、多客户端遗忘。三是性能好。那就是<font color="#ff0000">一好像厉害，二还行，三凑数</font>
+ 贡献纯扯淡

# Related Work
pass

# Method
这个方法需要目标客户端参与遗忘，去为辅助轻量级模块助力。该模块有两个特征：他是边学边忘的，该模块可以多个客户端独立训练或协作训练。
首先是轻量级模块：无论哪种场景，数据集都可以分为剩余和目标，对于目标数据集会移动正确标签，然后对那些参数单独训练。思路好像是把目标数据认为是已学习的数据。
实际上好像是对不同的场景有不同的线性设计方法，而不是一个大一统方法。