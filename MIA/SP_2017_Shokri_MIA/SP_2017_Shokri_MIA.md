#CCF-A #开源 

# Authors
康奈尔大学的goat级别团队
![[Pasted image 20250422211208.png]]

# Abstract
动机：本文量化了机器学习模型泄露数据的程度。

方法：本文聚焦于最基本的成员推理：用一个黑盒判断数据是否属于模型训练集。具体来说就是训练一个对抗性的训练一个推理模型，用来识别和目标模型对训练输入的预测差异。

结果：在谷歌和亚马逊提供的机器学习服务上进行了评估。使用了逼真、敏感（出院）的分类数据集进行测评证明有效，然后评估了缓解方法。

# Introduction
1. 机器学习应用广泛，用户各种数据都被用以训练。
2. 谷歌、亚马逊等巨头提供了MLAS服务，任何用户都可以上传数据集训练模型
3. 直接到贡献了？
	1. 提出了概念“成员推理 Member Inference”，给定一个模型和一条记录去判断该记录是否被用训练。作者们甚至在最困难的环境——黑盒中实现了该功能，总之就是通过预测模型的输出来量化泄露的信息。




# Comment
读本文的用处：首先这是一篇经典文章，有助于理解FU中常用的MIA指标，可以以此为基础