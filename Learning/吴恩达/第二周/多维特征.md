概念：feature，target

### 多维特征 Multiple features(Variables)

一些行向量，列向量的书写方式。<font color="#ff0000">箭头用来强调这是一个向量而不是数字</font>
![[Pasted image 20240610112346.png|550]]


### 向量化 Vectorization
使用向量化代码可以让代表更加简洁、高效，并有效利用GPU，典型代表：NumPy 

三种循环方式的对比：
+ 红脸：显然弱智写法
+ 紫脸：串行写法，数量一大就GG
+ <font color="#ff0000">笑脸：①写法简洁 ②可以调用平行计算的GPU</font>

![[Pasted image 20240610141336.png]]

是否有向量化在硬件上运行的差别：
一个是串行，另一个是并行再拼起来
![[Pasted image 20240610141917.png|375]]


### 多元线性回归的梯度下降 Gradient Descent for Multiple Regression

一张图言明，就是对于不同的特征，有自己的计算方式
![[Pasted image 20240610144316.png|475]]

介绍了一种叫做Normal equation的回归方法
1. 特点：只能用于线性回归
2. 优点：可以快速找出w和b，而不需要迭代
3. 缺点：无法适用在其它算法，特征多也会很慢
4. 简述：会调库就行

### 特征放缩 Feature Scaling
它可以让梯度下降的更快。

感觉就是权重的选择。合理的放缩可以让特征像个circle而不是oval

![[Pasted image 20240610155229.png]]

##### 如何进行特征放缩？
第一种方法：除以最大值
比如x1∈[300,2000]，我们令x1 = x1/2000
x2∈[0,5]，我们令x2=x2/5

<font color="#ff0000">第二种方法：Mean normalization，均值归一化</font>
范围最后会变成-1，1
μ就是均值，假设某些参数列表下均值μ1=600，计算如图
设μ2=2.3
![[Pasted image 20240610161017.png|400]]

第三种方法：Z-score normalization
设σ=450，μ=600
![[Pasted image 20240610161959.png|450]]


<font color="#ff0000">特征的放缩，取决于你工作的需要，就像那么多loss算法一样</font>


### Checking GD for Convergence 检查梯度下降是否收敛
重温一下梯度下降公式：关键是要选对α
![[Pasted image 20240610163630.png|200]]

即使是Andrew，也没有办法预估多少轮会收敛，一般会引入一个ε做临界值。如果梯度下降的值小于ε，就认为收敛了。（称为Automatic convergence test）
<font color="#ff0000">但是同理，ε也很难选择，左边的图表将会是一个更好的选择</font>
![[Pasted image 20240610164511.png|400]]


### Choosing the Learning Rate 选择学习率

如果代码出现图一的情况，可能是bug或者学习率太大。
通常来说损失函数肯定是下降的，<font color="#ff0000">因此Andrew分享了一个trick</font>：设置一个非常小的学习率，去看loss是否有下降来判断是bug还是lr太小。
![[Pasted image 20240610204328.png|550]]

### 选择合适的lr
三倍三倍的选，用少轮次迭代，得到loss下降最快且稳定的lr。（在前几轮loss就有震荡说明lr太大了）
0.001 - 0.003 - 0.01 - 0.03 - 0.1 ....


### 特征工程
选择合适的特征，对算法至关重要。例如房子的长、宽、面积，显然面积这个特征最为重要，最适合作为特征。

### 多项式回归 Polynomial Regression

当特征够多的时候，我们就可以开始用平方、立方这些概念。但要注意**特征的缩放** 
