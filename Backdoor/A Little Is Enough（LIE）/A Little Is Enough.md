
**本文有一个非常变态的地方，Arxiv版本比正式版完整**
# Authors
Moran Baruch 巴伊兰大学，资料较少

# Abstract
**背景：** 分布式学习会受到拜占庭参与者的攻击导致学习过程被中断或操控。
**动机**：过去的模型假设攻击者a）全知全能，意味着攻击者知道其他人的数据 b）大幅改变了参数，显然各种统计学的防御就可以抵御。
**方法**：攻击者利用这些经验主义的界限来攻击，甚至可以让服务器一直选择恶意参与者。
**结果**：该方法不仅可以阻止收敛，也适用于后门。MNIST和CIFAR-10需要25%，CIFAR100需要50%。

# Introduction
+ 本文更喜欢把FL叫分布式学习，客户端叫Worker。
+ 大多数分布式学习假设的环境是iid，且大部分防御方法基于良性客户端方差上界为限来排除恶意客户端
+ 本文不赞同该假设（我都不知道它假设了什么），反正LIE可以绕过SOTA，并且支持后门

贡献总结：
1. 提供了一个扰动范围，即使iid设置也检测不出来（可noniid根本无法检测）
2. 该范围内的变化足以干扰收敛和植入后门
3. 首次在iid设置下提出非全知全能的攻击
4. 该方法克服了所有SOTA防御（2019）


# Background
给了同步SGD的算法，**注意n是所有客户端，m是恶意客户端，d是维度，pi是客户端i的参数**

介绍了恶意者的目的：①阻止收敛 ②后门攻击

基于编码方式防御拜占庭攻击（pass）

基于统计方式防御拜占庭攻击：![[各种聚合]]

# Method
正态分布，单位用σ，从直觉上来说良性客户端应该是围绕均值对称的，本文提出的方法会让攻击者更加靠近均值

![[Pasted image 20241210222140.png]]

### 利用假设
过去阻止收敛的方法都会选择梯度相反的方向，这有效，但也容易被检测。

然后列举了三个聚合方法存在的缺点，反正就是会被攻克。同时指出**哪怕是很简单的任务比如MNIST，它的分布也存在很高的方差**，这意味着如果把恶意者数据稍微拉近一点，完全是会在bound之内，被视为良性。由于正态分布，只需要控制一部分就可以改变模型走向。

**因此本文的方式是对许多参数进行小改变，而不是对少数参数进行大改变。**

### 扰动范围
常规聚合防御方法旨在丢弃距离平均值太远的值，因此寻求一个合理的范围，由于正态分布是对称的，因此 **zmax 将设置平均值周围适用变化的下限和上限**。

### Z值计算
**举例了Z值的计算，看懂了** ，就是先得出值Z，然后在Z表里找小于等于这个数的最大值，具体eg看原文、
原文为0.923，对应Z值约1.43
我的值为为0.8333，对应Z值为0.96-0.97
### 后门攻击
经典的α：这个loss文章用MSE，我觉得根据需要调整
![[Pasted image 20241211175257.png]]
# Experiments
提供了两种实验（没看懂这两类是什么）
1. empirically validating the claim regarding the variance between correct workers
2. 通过攻击现实世界网络来验证方法的适用性。


### Setup
具体参数都在补充材料
数据集：MNIST & CIFAR10/简单结构，CIFAR100/WideResNet
客户端数量：51个，其中12个恶意，比例约等于24%

### Variance Between Correct Workers
这部分采用的攻击叫做gradient sign-flipping，每一轮攻击者反转部分梯度符号。