### 动机
即使是采用了参数分解PFL，依然会受到后门攻击

### 方法
分类器异质性：这个第一点感觉可以用cifar10，选3类出来模拟一下
1. 数据异构，分类器对干净 | 中毒数据分类明显
2. 投毒会加速恶意和良性客户端的分离
良性客户端与恶意客户端的决策边界明显不同—> 对有毒样本特征表达的部分投毒的同时保持分类器的稳定，尽可能减小对自己的干扰？
[【论文阅读笔记】BapFL:You can backdoor personalized federated learning-CSDN博客](https://blog.csdn.net/leticia_m/article/details/136816515)

先别想那么多，如果它的加噪样本归类成功了，是不是直接进行遗忘，就完成了对某一个子类攻击？但是这样好像并不能满足对某个客户端攻击，我们需要知道这个客户端所有的class的分类所在。



### 实验


### 展望


### 基础内容学习
PFL的参数分解一般有两种
1. 分享编码器层，保留分类器层
2. 保留Batch Norm层，分享其他所有层。

### 代码部分
1. 有计算参数量的代码
2. 加噪声的代码